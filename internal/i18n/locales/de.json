{
  "app.version": "ollama-remote {version}",
  "app.version_commit": "ollama-remote {version} ({commit})",

  "help.usage": "Verwendung: {app} [--host <url>] [--lang <en|es|de>] [--ollama-exe <pfad>] [--mode <auto|wrapper|native>] [--unsafe] [--config <pfad>] <befehl|ollama-args...>",
  "help.what_is": "Ein kleiner Wrapper, der die offizielle Ollama-CLI mit dem konfigurierten OLLAMA_HOST ausfuhrt.",
  "help.global_flags": "Globale Flags:",
  "help.flag.host": "  --host <url>          OLLAMA_HOST fur diesen Aufruf uberschreiben",
  "help.flag.lang": "  --lang <en|es|de>     Sprache fur die Ausgabe dieses Tools",
  "help.flag.ollama_exe": "  --ollama-exe <pfad>  Pfad zur Ollama-CLI (sonst PATH)",
  "help.flag.mode": "  --mode <auto|wrapper|native>  Ausfuhrungsmodus (Standard: auto)",
  "help.flag.unsafe": "  --unsafe              Erlaubt mutierende/fortgeschrittene Operationen im nativen Modus",
  "help.flag.config": "  --config <pfad>       Nur diese Konfiguration nutzen (kein Auto-Discovery)",
  "help.flag.help": "  -h, --help            Diese Hilfe anzeigen",
  "help.flag.version": "  --version             Version anzeigen",
  "help.wrapper_cmds": "Wrapper-Befehle:",
  "help.cmd.config": "  config [show|set|init|path]  Benutzer-Konfiguration verwalten",
  "help.cmd.doctor": "  doctor                      Grundlegendes Setup prufen",
  "help.cmd.ui": "  ui                          Optionale lokale Web-UI starten",
  "help.examples": "Beispiele:",
  "help.example.list": "  ollama-remote list",
  "help.example.run": "  ollama-remote run llama3:8b",
  "help.example.host": "  ollama-remote --host https://ollama.example.com:11434 ps",
  "help.example.lang": "  ollama-remote --lang de doctor",
  "help.example.ui": "  ollama-remote ui",
  "help.try_help": "Versuch: {app} --help",

  "config.path": "Konfigurationspfad: {path}",
  "config.host": "host = {value}",
  "config.lang": "lang = {value}",
  "config.ollama_exe": "ollama_exe = {value}",
  "config.mode": "mode = {value}",
  "config.no_proxy_auto": "no_proxy_auto = {value}",
  "config.unsafe": "unsafe = {value}",
  "config.value.auto": "auto",
  "config.inited": "Konfiguration erstellt: {path}",
  "config.set_ok": "Aktualisiert: {key}",

  "doctor.host": "Host: {value}",
  "doctor.lang": "Sprache: {value}",
  "doctor.mode": "Modus: {value}",
  "doctor.unsafe": "Unsafe: {value}",
  "doctor.selected_mode": "Ausgewahlter Modus: {value}",
  "doctor.api_version": "API-Version: {value}",
  "doctor.api_version_failed": "API-Version-Prufung fehlgeschlagen: {error}",
  "doctor.ollama_cli": "Ollama-CLI: {value}",
  "doctor.value.not_found": "nicht gefunden",
  "doctor.ollama_failed": "Ollama konnte nicht ausgefuhrt werden: {error}",

  "ui.started": "UI: {url}",
  "ui.stop_hint": "Mit Ctrl+C beenden.",

  "ui.subtitle": "Lokale UI fur die offizielle Ollama-CLI. Laeuft nur auf deinem Rechner.",
  "ui.section.config": "Konfiguration",
  "ui.section.pull": "Modell laden",
  "ui.section.run": "Prompt ausfuhren",
  "ui.section.output": "Ausgabe",
  "ui.label.host": "Host",
  "ui.label.language": "Sprache",
  "ui.label.model": "Modell",
  "ui.label.prompt": "Prompt",
  "ui.btn.list": "Modelle anzeigen",
  "ui.btn.save": "Speichern",
  "ui.btn.pull": "Laden",
  "ui.btn.run": "Ausfuhren",
  "ui.placeholder.host": "http://127.0.0.1:11434",
  "ui.placeholder.model": "llama3:8b",
  "ui.placeholder.prompt": "Schreibe einen kurzen Incident-Postmortem...",
  "ui.message.saved_restart": "Gespeichert. Starte die UI neu, um Sprachanderungen vollstandig anzuwenden.",
  "ui.error.unauthorized": "Nicht autorisiert",
  "ui.error.bad_request": "Ungueltige Anfrage",
  "ui.error.model_required": "Modell ist erforderlich",

  "error.invalid_args": "Fehler: {error}",
  "error.arg.unknown_flag": "Unbekanntes Flag: {flag}",
  "error.arg.missing_value": "{flag} erwartet einen Wert",
  "error.invalid_mode": "Ungueltiger Modus: {mode} (erwartet: auto, wrapper, native)",
  "error.invalid_host": "Ungueltiger Host: {host} ({error})",
  "error.invalid_ollama_exe": "Ungueltiges --ollama-exe / OLLAMA_EXE (nicht gefunden): {path}",
  "error.unknown_subcommand": "Unbekannter Subcommand: {sub}",
  "error.config_load": "Konfiguration konnte nicht geladen werden ({path}): {error}",
  "error.config_init": "Konfiguration konnte nicht erstellt werden ({path}): {error}",
  "error.config_set_usage": "Verwendung: ollama-remote config set <host|lang|ollama_exe|mode|no_proxy_auto|unsafe> <wert>",
  "error.config_unknown_key": "Unbekannter Konfigurationsschlussel: {key}",
  "error.config_set": "Konfiguration konnte nicht aktualisiert werden: {error}",
  "error.env_build": "Umgebung konnte nicht vorbereitet werden: {error}",
  "error.ollama_not_found": "Ollama-CLI nicht gefunden. Bitte Ollama installieren oder OLLAMA_EXE setzen. {hint}",
  "error.ollama_failed": "Ollama fehlgeschlagen: {error}",
  "error.ui_listen": "Konnte nicht lauschen: {error}",
  "error.ui_start": "UI konnte nicht gestartet werden: {error}",

  "error.native.usage_run": "Verwendung (nativ): ollama-remote run <modell> [--] <prompt> (oder Prompt per stdin)",
  "error.native.run_requires_prompt": "Der native Modus erfordert einen expliziten Prompt (Arg oder stdin). Interaktiver Run erfordert wrapper.",
  "error.native.usage_pull": "Verwendung (nativ): ollama-remote pull <modell>",
  "error.native.pull_requires_unsafe": "pull ist im nativen Modus standardmassig deaktiviert. Erneut mit --unsafe (oder unsafe=true).",
  "error.native.usage_show": "Verwendung (nativ): ollama-remote show <modell>",
  "error.native.unsupported": "Nicht unterstutzt im nativen Modus: {cmd} (Ollama-CLI installieren oder --mode=wrapper nutzen)"
}
